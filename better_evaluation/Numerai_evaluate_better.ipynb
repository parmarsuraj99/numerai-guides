{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerai_evaluate_better.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG/J/39tFnDEE2J3gYVNHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parmarsuraj99/numerai-guides/blob/master/better_evaluation/Numerai_evaluate_better.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjYh7u4jsWul",
        "colab_type": "text"
      },
      "source": [
        "Created by Suraj Parmar\n",
        "\n",
        "- Numerai: [SurajP](https://numer.ai/surajp)\n",
        "\n",
        "- Twitter: [@parmarsuraj99](https://twitter.com/parmarsuraj99)\n",
        "\n",
        "\n",
        "Thanks to [@NJ](https://twitter.com/tasha_jade) and [@MikeP](https://twitter.com/EasyMikeP) for their guidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esSvK3mcccGn",
        "colab_type": "text"
      },
      "source": [
        "# Don't just submit and wait. Evaluate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lyfeOgwzDiD",
        "colab_type": "text"
      },
      "source": [
        "A walkthrough to some evaluation metrics you can use for your models. \n",
        "\n",
        "This isn't ment to be a `Run All` notebook. I have tried to make this flexible and customizable so that you can adapt this to your workflow.\n",
        "\n",
        "Hope this helps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTc0OJEJ77Sb",
        "colab_type": "text"
      },
      "source": [
        "## 1.0 Loading required libraries üìî and dataset üóÑÔ∏èüîΩ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dukzbOx5YPL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numerapi\n",
        "!pip install catboost;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3qA9k0VZ4Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import numerapi\n",
        "\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrzPVfR6egjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "# download current dataset\n",
        "napi.download_current_dataset(unzip=True)\n",
        "\n",
        "current_ds = napi.get_current_round()\n",
        "latest_round = os.path.join('numerai_dataset_'+str(current_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHATa1jEq01J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOURNAMENT_NAME = \"kazutsugi\"\n",
        "TARGET_NAME = f\"target_{TOURNAMENT_NAME}\"\n",
        "PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n",
        "\n",
        "BENCHMARK = 0\n",
        "BAND = 0.2\n",
        "\n",
        "#-----------------------------------------------------\n",
        "\n",
        "# Submissions are scored by spearman correlation\n",
        "def score(df):\n",
        "    # method=\"first\" breaks ties based on order in array\n",
        "    return np.corrcoef(\n",
        "        df[TARGET_NAME],\n",
        "        df[PREDICTION_NAME].rank(pct=True, method=\"first\")\n",
        "    )[0, 1]\n",
        "\n",
        "\n",
        "# The payout function\n",
        "def payout(scores):\n",
        "    return ((scores - BENCHMARK) / BAND).clip(lower=-1, upper=1)\n",
        "\n",
        "\n",
        "# Read the csv file into a pandas Dataframe\n",
        "def read_csv(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        column_names = next(csv.reader(f))\n",
        "        dtypes = {x: np.float16 for x in column_names if\n",
        "                  x.startswith(('feature', 'target'))}\n",
        "    return pd.read_csv(file_path, dtype=dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgodzImqq7z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "print(\"# Loading data...\")\n",
        "# The training data is used to train your model how to predict the targets.\n",
        "training_data = read_csv(os.path.join(latest_round, \"numerai_training_data.csv\")).set_index(\"id\")\n",
        "# The tournament data is the data that Numerai uses to evaluate your model.\n",
        "tournament_data = read_csv(os.path.join(latest_round, \"numerai_tournament_data.csv\")).set_index(\"id\")\n",
        "\n",
        "example_preds = read_csv(os.path.join(latest_round, \"example_predictions_target_kazutsugi.csv\")).set_index(\"id\")\n",
        "\n",
        "validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKGYq1Ce7MHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [f for f in training_data.columns if f.startswith(\"feature\")]\n",
        "print(f\"Loaded {len(feature_names)} features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWLPUTla61YQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_EVAL_PREFIX = \"train\"\n",
        "VAL_EVAL_PREFIX = \"val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9HQWh6zsJcH",
        "colab_type": "text"
      },
      "source": [
        "## <center>Notebook Flow</center>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/parmarsuraj99/numerai-guides/master/better_evaluation/images/notebook_flow.png\" width=40%>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "## <center>Some simple metrics to get started</center>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/parmarsuraj99/numerai-guides/master/better_evaluation/images/simple_stats.png\" width=70%\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9gwbJqJpiW",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Some added metrics for evaluation üî¨\n",
        "\n",
        "This is just to get familiar with some simpler metrics. Functions with all the metrics are in the section 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RScdwXCNJ7m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spearmanr(target, pred):\n",
        "    return np.corrcoef(\n",
        "        target,\n",
        "        pred.rank(pct=True, method=\"first\")\n",
        "    )[0, 1]\n",
        "\n",
        "def get_basic_per_era_metrics(df:pd.DataFrame, \n",
        "                        isVal=None, \n",
        "                        fig_name=\"per_era_scores.png\") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Some metrics related to per-era scores.\n",
        "    Plots per-era mean correlation with `TARGET_NAME` column\n",
        "    more metrics at: https://forum.numer.ai/t/more-metrics-for-ya/636\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_with_preds : pd.DataFrame\n",
        "        Training or Toutnament DataFrame having predictions assigned\n",
        "        at `PREDICTION_NAME` column.\n",
        "\n",
        "    isVal: bool, optional, default:None\n",
        "    \n",
        "    fig_name: str, optional, default:per_era_scores.png\n",
        "\n",
        "    Returns:\n",
        "    ------\n",
        "    pd.Series: Pandas Series having \n",
        "        (mean, std, %preds<0.5, and %preds<mean(preds), corr w/ example_preds)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    prefix=None\n",
        "    #Some checks for deciding between training and tournament data\n",
        "    if isVal:\n",
        "        df = df[df.data_type == \"validation\"]\n",
        "        prefix=VAL_EVAL_PREFIX\n",
        "        print(\"predicting on validation...\")\n",
        "    else:\n",
        "        df = df\n",
        "        prefix=TRAIN_EVAL_PREFIX\n",
        "        print(\"predicting on train...\")\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    scores = pd.Series(dtype=float)\n",
        "    preds_ = df[PREDICTION_NAME]\n",
        "    scores[f\"{prefix}_mean\"] = preds_.mean()\n",
        "    scores[f\"{prefix}_std_dev\"] = preds_.std()\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    #Metric Calculations\n",
        "    print(\"getting per era scores\")\n",
        "    era_scores = df.groupby(\"era\").apply(\n",
        "        lambda x: spearmanr(x[TARGET_NAME], x[PREDICTION_NAME]))\n",
        "    \n",
        "    era_scores.sort_index(inplace=True)\n",
        "    era_scores.plot(kind=\"bar\")\n",
        "    print(\"performance over time\")\n",
        "    plt.savefig(f\"{prefix}_{fig_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    scores[f\"{prefix}_mean correlation\"] = np.mean(era_scores)\n",
        "    scores[f\"{prefix}_Std. Dev.\"] = np.std(era_scores)\n",
        "    scores[f\"{prefix}_sharpe\"] = np.mean(era_scores)/np.std(era_scores)\n",
        "\n",
        "    print(scores)\n",
        "    del era_scores\n",
        "    del preds_\n",
        "    \n",
        "    gc.collect()\n",
        "    return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okx5_Wi4khIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_metrics(model, \n",
        "                    feature_names:list=feature_names, \n",
        "                    fig_name=\"per_era_scores\")->pd.Series:\n",
        "\n",
        "    training_preds = model.predict(training_data[feature_names].values)\n",
        "    training_data[PREDICTION_NAME] = np.array(training_preds).reshape(-1,1)\n",
        "\n",
        "    tournament_preds = model.predict(tournament_data[feature_names].values)\n",
        "    tournament_data[PREDICTION_NAME] = np.array(tournament_preds).reshape(-1,1)\n",
        "\n",
        "    del training_preds\n",
        "    del tournament_preds\n",
        "\n",
        "    print(\"evaluating on training data...\")\n",
        "    tr_per_era_scores = get_basic_per_era_metrics(training_data, isVal=False, fig_name=fig_name)\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"evaluating on validation data...\")\n",
        "    val_per_era_scores = get_basic_per_era_metrics(tournament_data, isVal=True, fig_name=fig_name)\n",
        "    gc.collect()\n",
        "\n",
        "    return pd.concat([\n",
        "                      tr_per_era_scores, val_per_era_scores,\n",
        "                      ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwdqS642BP-u",
        "colab_type": "text"
      },
      "source": [
        "### Creating some models ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiy3tWF07pgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = dict()\n",
        "\n",
        "#Linear model\n",
        "lin_reg = LinearRegression()\n",
        "models[\"lin_reg\"] = lin_reg\n",
        "\n",
        "#Neural Net\n",
        "nn_model = tf.keras.models.Sequential([\n",
        "                                       tf.keras.layers.Input(shape=(310,)),\n",
        "                                       tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "                                       tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "nn_model.compile(loss=\"mse\", optimizer=\"adam\", metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "models[\"keras_mlp_simple\"] = nn_model\n",
        "\n",
        "del lin_reg\n",
        "del nn_model\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjcpRy9a0KVS",
        "colab_type": "text"
      },
      "source": [
        "### Training our models‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42sdknvqBg2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model_name in models:\n",
        "    print(f\"Fitting {model_name}...\")\n",
        "\n",
        "    if \"keras\" in model_name:\n",
        "        models[model_name].fit(training_data[feature_names].values, training_data[TARGET_NAME].values, \n",
        "             batch_size=512, \n",
        "             epochs=40,\n",
        "             validation_data=(validation_data[feature_names].values, validation_data[TARGET_NAME].values),\n",
        "             )\n",
        "    else:\n",
        "        models[model_name].fit(training_data[feature_names].values, training_data[TARGET_NAME].values)\n",
        "\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxXQTyRFtZxo",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating models üî¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM563EctEM0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "all_model_metrics = dict()\n",
        "for model_name in models:\n",
        "    \n",
        "    print(f\"\\n----{model_name}----\")\n",
        "    model_metrics = get_all_metrics(models[model_name], feature_names, fig_name = f\"{model_name}.png\")\n",
        "    all_model_metrics[model_name] = model_metrics\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C98Yl9f-BglX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_df = pd.DataFrame.from_dict(all_model_metrics)\n",
        "metric_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLl9fpob7kZp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- **`val_mean_correlation`** suggests how much your predictions are correlated to targets across `era`\n",
        "- **`val_sharpe`** (mean/std) is for higher mean with lower std. dev.\n",
        "- **`tournament_corr_example_preds`** shows how much your predictions are correlated with `example_predictions` (which have shown very good performance)\n",
        "\n",
        "As you can see, Neural Net is showing better performance here based on these metrics. Now, let's compare Neural Net with default CatBoost model on more metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wUBmelhyb6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## <center>Metrics used here</center>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/parmarsuraj99/numerai-guides/master/better_evaluation/images/all_metrics.png\" width=100%\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxzqRqTxQjGN",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Expanding the vision with more metrics üî≠\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvC0ylq15cdn",
        "colab_type": "text"
      },
      "source": [
        "Do check out [More Metrics for ya](https://forum.numer.ai/t/more-metrics-for-ya/636) for community discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WInhlaAl60of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_EVAL_PREFIX = \"train\"\n",
        "VAL_EVAL_PREFIX = \"val\"\n",
        "\n",
        "#Some evaluation metrics\n",
        "def ar1(x):\n",
        "    return np.corrcoef(x[:-1], x[1:])[0,1]\n",
        "\n",
        "def autocorr_penalty(x):\n",
        "    n = len(x)\n",
        "    p = ar1(x)\n",
        "    return np.sqrt(1 + 2*np.sum([((n - i)/n)*p**i for i in range(1,n)]))\n",
        "\n",
        "def smart_sharpe(x):\n",
        "    return np.mean(x)/(np.std(x, ddof=1)*autocorr_penalty(x))\n",
        "\n",
        "def numerai_sharpe(x):\n",
        "    return ((np.mean(x) - 0.010415154) / np.std(x)) * np.sqrt(12)\n",
        "\n",
        "def spearmanr(target, pred):\n",
        "    return np.corrcoef(\n",
        "        target,\n",
        "        pred.rank(pct=True, method=\"first\")\n",
        "    )[0, 1]\n",
        "\n",
        "#-----------------------------------------------------\n",
        "def get_baisc_per_era_metrics(df:pd.DataFrame, \n",
        "                        isVal=None, \n",
        "                        fig_name=\"per_era_scores.png\") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Some metrics related to per-era scores.\n",
        "    Plots per-era mean correlation with `TARGET_NAME` column\n",
        "\n",
        "    more metrics at: https://forum.numer.ai/t/more-metrics-for-ya/636\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_with_preds : pd.DataFrame\n",
        "        Training or Toutnament DataFrame having predictions assigned\n",
        "        at `PREDICTION_NAME` column.\n",
        "\n",
        "    isVal: bool, optional, default:None\n",
        "        Indication of DataFrame having Validation data. However, the \n",
        "        function checks for this is `isVal=None`. Saves checking time!\n",
        "    \n",
        "    fig_name: str, optional, default:per_era_scores.png\n",
        "        Name for per-era correlation graph to be saved with extension.\n",
        "        prefix will be added to the file name based on `data_type`.\n",
        "    Returns:\n",
        "    ------\n",
        "    pd.Series: Pandas Series having per-era metriccs\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    prefix=None\n",
        "    scores = pd.Series(dtype=float)\n",
        "\n",
        "    preds_ = df[PREDICTION_NAME]\n",
        "    #Some checks for deciding between training and tournament data\n",
        "    if isVal:\n",
        "        scores[\"tournament_corr_example_preds\"] = spearmanr(preds_, example_preds[PREDICTION_NAME])\n",
        "        df = df[df.data_type == \"validation\"]\n",
        "        prefix=VAL_EVAL_PREFIX\n",
        "        print(\"predicting on validation...\")\n",
        "    else:\n",
        "        df = df\n",
        "        prefix=TRAIN_EVAL_PREFIX\n",
        "        print(\"predicting on train...\")\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "\n",
        "    #Metric Calculations\n",
        "    print(\"getting per era scores\")\n",
        "    era_scores = df.groupby(\"era\").apply(\n",
        "        lambda x: spearmanr(x[TARGET_NAME], x[PREDICTION_NAME]))\n",
        "    \n",
        "    era_scores.sort_index(inplace=True)\n",
        "    era_scores.plot(kind=\"bar\")\n",
        "    print(\"performance over time\")\n",
        "    plt.savefig(f\"{prefix}_{fig_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    \n",
        "    scores[f\"{prefix}_mean\"] = preds_.mean()\n",
        "    scores[f\"{prefix}_std_dev\"] = preds_.std()\n",
        "    scores[f\"{prefix}_less_than_half\"] = (preds_<0.5).mean()\n",
        "    scores[f\"{prefix}_less_than_mean\"] = (preds_<preds_.mean()).mean()\n",
        "\n",
        "    scores[f\"{prefix}_autocorrelation\"] = ar1(era_scores)\n",
        "    scores[f\"{prefix}_mean correlation\"] = np.mean(era_scores)\n",
        "    scores[f\"{prefix}_Median Correlation\"] = np.median(era_scores)\n",
        "    scores[f\"{prefix}_Variance\"] = np.var(era_scores)\n",
        "    scores[f\"{prefix}_Std. Dev.\"] = np.std(era_scores)\n",
        "    scores[f\"{prefix}_sharpe\"] = np.mean(era_scores)/np.std(era_scores)\n",
        "    scores[f\"{prefix}_smart sharpe\"] = smart_sharpe(era_scores)\n",
        "    scores[f\"{prefix}_Numerai sharpe\"] = numerai_sharpe(era_scores)\n",
        "\n",
        "    print(scores)\n",
        "    del era_scores\n",
        "    del preds_\n",
        "    gc.collect()\n",
        "    return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9A4b0HQ60kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inspired from https://forum.numer.ai/t/more-metrics-for-ya/636\n",
        "#and https://github.com/numerai/example-scripts/blob/master/era_boosting_example.ipynb\n",
        "#and help from `MikeP`\n",
        "\n",
        "def neutralize(df, columns, by, proportion=1.0):\n",
        "    scores = df[columns]\n",
        "    exposures = df[by].values\n",
        "    \n",
        "    # constant column to make sure the series is completely neutral to exposures\n",
        "    exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n",
        "    gc.collect()\n",
        "    scores = scores - proportion * exposures.dot(np.linalg.pinv(exposures).dot(scores))\n",
        "    gc.collect()\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "def calculate_feature_exposure(df, feature_names) -> list:\n",
        "    exposures = []\n",
        "    for feature_name in feature_names:\n",
        "        exposures.append(spearmanr(df[feature_name], df[PREDICTION_NAME]))\n",
        "        \n",
        "    max_feat_exposure = np.max(np.abs(exposures))\n",
        "    square_sum_feature_exposure = np.sum([e**2 for e in exposures])\n",
        "    feature_exposure = np.std(exposures)\n",
        "\n",
        "    #print(max_feat_exposure, square_sum_feature_exposure)\n",
        "\n",
        "    return [feature_exposure, max_feat_exposure, square_sum_feature_exposure]\n",
        "\n",
        "\n",
        "def get_more_metrics(df, feature_names, isVal=None) -> pd.Series:\n",
        "    \"\"\"\n",
        "    more metrics at: https://forum.numer.ai/t/more-metrics-for-ya/636\n",
        "\n",
        "    Returns basic stats about predictions in PREDICTION_NAME columns.\n",
        "    It checks for training or tournament and validation predictions\n",
        "\n",
        "    Args:\n",
        "    ------\n",
        "    df (pd.DataFrame): Training or Tournament DataFrame after assigning predictions\n",
        "    feature_names (list): List of features to use for neutralization. (Here, all)\n",
        "    isVal (bool): Boolean suggesting that df contains validation_data\n",
        "\n",
        "    Returns:\n",
        "    ------\n",
        "    pd.Series: Pandas Series having \n",
        "        (var, feature_neutral_mean, feat_exposure, max_feat_exposure, \n",
        "        square_sum_feature_exposure, max_drawdown)\n",
        "    \"\"\"\n",
        "\n",
        "    more_metrics = pd.Series(dtype=float)\n",
        "    metric_prefix=None\n",
        "    assert PREDICTION_NAME in df.columns\n",
        "\n",
        "    if isVal is None:\n",
        "        isVal = \"validation\" in df[\"data_type\"].unique() #max CPU times: user 65.1 ms\n",
        "\n",
        "    print(isVal)\n",
        "    if isVal:\n",
        "        df = df[df[\"data_type\"]==\"validation\"]\n",
        "        metric_prefix = VAL_EVAL_PREFIX\n",
        "    else:\n",
        "        metric_prefix = TRAIN_EVAL_PREFIX\n",
        "\n",
        "    assert metric_prefix is not None\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "\n",
        "    #per-era scores\n",
        "    \"\"\"\n",
        "    per-era scores\n",
        "    \"\"\"\n",
        "    print(\"predicting per-era scores...\")\n",
        "    scores_per_era = df.groupby(\"era\").apply(\n",
        "        lambda df: spearmanr(df[PREDICTION_NAME], df[TARGET_NAME]))\n",
        "    \n",
        "    more_metrics[f\"{metric_prefix}_var\"] = scores_per_era.std()\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    \n",
        "    #Neutralize\n",
        "    #This takes a significant amount of memory for calculation\n",
        "    print(df.shape)\n",
        "    print(\"Neutralizing...\")\n",
        "    df[f\"neutral_{PREDICTION_NAME}\"] = neutralize(df, PREDICTION_NAME, feature_names)\n",
        "    feature_neutral_mean = df.groupby(\"era\").apply(\n",
        "        lambda x: spearmanr(x[\"neutral_\"+PREDICTION_NAME].values, x[TARGET_NAME])).mean()\n",
        "\n",
        "    more_metrics[f\"{metric_prefix}_feature_neutral_mean\"] = feature_neutral_mean\n",
        "    gc.collect()\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    print(\"Calculating Feature Exposure...\")\n",
        "    feature_exposure, max_feat_exposure, square_sum_feature_exposure = calculate_feature_exposure(df, feature_names)\n",
        "\n",
        "    more_metrics[f\"{metric_prefix}_feat_exposure\"] = feature_exposure\n",
        "    more_metrics[f\"{metric_prefix}_max_feat_exposure\"] = max_feat_exposure\n",
        "    more_metrics[f\"{metric_prefix}_square_sum_feature_exposure\"] = square_sum_feature_exposure\n",
        "\n",
        "\n",
        "    #-----------------------------------------------------\n",
        "    print(\"Drawdown...\")\n",
        "    rolling_max = (scores_per_era+1).cumprod().rolling(window=100, min_periods=1).max()\n",
        "    daily_value = (scores_per_era+1).cumprod()\n",
        "    max_drawdown = (rolling_max - daily_value).max()\n",
        "\n",
        "    more_metrics[f\"{metric_prefix}_max_drawdown\"] = max_drawdown\n",
        "\n",
        "    return more_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp9Gyg52DL6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_metrics(model, \n",
        "                    feature_names:list=feature_names, \n",
        "                    fig_name=\"per_era_scores\")->pd.Series:\n",
        "\n",
        "    training_preds = model.predict(training_data[feature_names].values)\n",
        "    training_data[PREDICTION_NAME] = np.array(training_preds).reshape(-1,1)\n",
        "\n",
        "    tournament_preds = model.predict(tournament_data[feature_names].values)\n",
        "    tournament_data[PREDICTION_NAME] = np.array(tournament_preds).reshape(-1,1)\n",
        "\n",
        "    del training_preds\n",
        "    del tournament_preds\n",
        "\n",
        "    print(\"evaluating on training data...\")\n",
        "    tr_per_era_scores = get_baisc_per_era_metrics(training_data, isVal=False, fig_name=fig_name)\n",
        "    tr_more_metrics = get_more_metrics(training_data, feature_names ,isVal=False)\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"evaluating on validation data...\")\n",
        "    val_per_era_scores = get_baisc_per_era_metrics(tournament_data, isVal=True, fig_name=fig_name)\n",
        "    val_more_metrics = get_more_metrics(tournament_data, feature_names ,isVal=True)\n",
        "    gc.collect()\n",
        "\n",
        "    return pd.concat([\n",
        "                      tr_per_era_scores, val_per_era_scores,\n",
        "                      tr_more_metrics, val_more_metrics,\n",
        "                      ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM6A6I2f0mCE",
        "colab_type": "text"
      },
      "source": [
        "### Creating some models ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Kl9Ov0BVRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = dict()\n",
        "\n",
        "#CatBoost Regressor\n",
        "cat_reg_params = {\n",
        "    'task_type': 'GPU'\n",
        "    }\n",
        "cat_regressor = CatBoostRegressor(**cat_reg_params)\n",
        "models[\"cat_reg\"] = cat_regressor\n",
        "\n",
        "#Neural Net\n",
        "nn_model = tf.keras.models.Sequential([\n",
        "                                       tf.keras.layers.Input(shape=(310,)),\n",
        "                                       tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "                                       tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "nn_model.compile(loss=\"mse\", optimizer=\"adam\", metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "models[\"keras_mlp_simple\"] = nn_model\n",
        "\n",
        "del cat_regressor\n",
        "del nn_model\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f7bnWnEIV3Zx"
      },
      "source": [
        "### Training our models‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D3jvshpC3hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model_name in models:\n",
        "    print(f\"Fitting {model_name}...\")\n",
        "\n",
        "    if \"keras\" in model_name:\n",
        "        models[model_name].fit(training_data[feature_names].values, training_data[TARGET_NAME].values, \n",
        "             batch_size=512, \n",
        "             epochs=40,\n",
        "             validation_data=(validation_data[feature_names].values, validation_data[TARGET_NAME].values),\n",
        "             )\n",
        "    else:\n",
        "        models[model_name].fit(training_data[feature_names].values, training_data[TARGET_NAME].values)\n",
        "\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCru_1GHtlqz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating models üî≠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOM9QWZ1J7aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "all_model_metrics = dict()\n",
        "for model_name in models:\n",
        "    \n",
        "    print(f\"\\n----{model_name}----\")\n",
        "    model_metrics = get_all_metrics(models[model_name], feature_names, fig_name = f\"{model_name}.png\")\n",
        "    all_model_metrics[model_name] = model_metrics\n",
        "    \n",
        "    gc.collect()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_GL_WXJ7VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_df = pd.DataFrame.from_dict(all_model_metrics)\n",
        "metric_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrjnNHXO5GXz",
        "colab_type": "text"
      },
      "source": [
        "### Let's Compare these models aginst `example_predictions`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi7LlKkwwO96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tournament_data[PREDICTION_NAME] = example_preds.values.reshape(-1, 1)\n",
        "\n",
        "example_val_per_era_scores = get_baisc_per_era_metrics(tournament_data, isVal=True, fig_name=\"example_preds\")\n",
        "example_val_more_metrics = get_more_metrics(tournament_data, feature_names ,isVal=True)\n",
        "\n",
        "example_preds_metrics = pd.concat([example_val_per_era_scores, example_val_more_metrics])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqB0H44FxrHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_df[\"example_preds\"] = example_preds_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbfpTa6y43Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcNmOGJE9HA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbFcAngDMwB_",
        "colab_type": "text"
      },
      "source": [
        "## Model for your submission\n",
        "\n",
        "Left as an exercise to the reader üòâ\n",
        "\n",
        "‚ñ∂Ô∏è Visit [An easy guide to ‚ÄúThe hardest data science tournament on the planet‚Äù](https://towardsdatascience.com/a-guide-to-the-hardest-data-science-tournament-on-the-planet-748f46e83690) to get started with how to make your submissions directly from colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYKGL_31Mz9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train and evaluate your model for submission\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6szh-CK1rmKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnR0L5j8p8Qg",
        "colab_type": "text"
      },
      "source": [
        "## Uploading predictions using your API keys üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNlyLZYYO-qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tournament_data[PREDICTION_NAME].to_csv(f\"{TOURNAMENT_NAME}_{current_ds}_submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEfqpxcEWDdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NameOfYourAI\n",
        "public_id = \"YourKeys\"\n",
        "secret_key = \"YourKeys\"\n",
        "model_id = \"YourKeys\"\n",
        "napi = numerapi.NumerAPI(public_id=public_id, secret_key=secret_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAIJHaoW3VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_id = napi.upload_predictions(f\"{TOURNAMENT_NAME}_{current_ds}_submission.csv\", model_id=model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzH2ACmbrp0m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiLIRZ_OmKP7",
        "colab_type": "text"
      },
      "source": [
        "# Whats Next?\n",
        " - **Customize this according to your workflow**\n",
        " - Read about [MMC](https://docs.numer.ai/tournament/metamodel-contribution)\n",
        " - Connect on [RocketChat](https://community.numer.ai/) or [Forum](https://forum.numer.ai/)\n",
        " - Take some help from [analysis_and_tips.ipynb](https://github.com/numerai/example-scripts/blob/master/analysis_and_tips.ipynb) notebook\n",
        " - Read this topic on forum about  metrics - [\"More Metrics for ya\"¬†](https://forum.numer.ai/t/more-metrics-for-ya/636)\n",
        " - Read and try to join the [weekly Office Hours](https://docs.numer.ai/office-hours-with-arbitrage/office-hours-recaps/ohwa-1) \\(I was interviewed [OHwA S02E10](https://docs.numer.ai/office-hours-with-arbitrage/office-hours-recaps-season-2/ohwa-s02e10) üòÉ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF9Q1rBEsyXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}